{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bce6f00",
   "metadata": {},
   "source": [
    "# Prompting LLMs for Patient-Oriented Language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870e8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TextStreamer\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a6cd0",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "988313a8-16e0-4bf5-b6df-0164f6079b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/linkhome/rech/genlig01/ulj12fo/.cache/huggingface/datasets/csv/default-c10b8bc08219fdf5/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|##########| 1/1 [00:00<00:00, 598.08it/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"csv\", data_files=\"pleaseee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677130fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'topic_id', 'statement_medical', 'statement_pol', 'premise', 'NCT_title', 'NCT_id', 'label'],\n",
      "    num_rows: 1578\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nstatements_pol = data['train']['statement_pol']\\nstatements_med = data['train']['statement_med']\\npremises =  data['train']['premise']\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = load_dataset(\"Mathilde/test_data_pol\")\n",
    "subset = data['train']  # TODO changer en test dans le ds original\n",
    "print(subset)\n",
    "\"\"\"\n",
    "statements_pol = data['train']['statement_pol']\n",
    "statements_med = data['train']['statement_med']\n",
    "premises =  data['train']['premise']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdb8cac",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "TODO: change the model's path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0a6c139",
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'OSError'>",
     "evalue": "Incorrect path_or_model_id: '../baselines/Qwen2.5-7B-Instruct'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2024.06/envs/pytorch-gpu-2.4.0+py3.11.5/lib/python3.11/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2024.06/envs/pytorch-gpu-2.4.0+py3.11.5/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2024.06/envs/pytorch-gpu-2.4.0+py3.11.5/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../baselines/Qwen2.5-7B-Instruct'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../baselines/Qwen2.5-7B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# \"/gpfsdswork/dataset/HuggingFace_Models/meta-llama/Llama-2-7b-chat-hf\" #\"/lustre/fsn1/projects/rech/hjp/ulj12fo/flan-t5-base\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2024.06/envs/pytorch-gpu-2.4.0+py3.11.5/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:834\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    836\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2024.06/envs/pytorch-gpu-2.4.0+py3.11.5/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:666\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    665\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 666\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2024.06/envs/pytorch-gpu-2.4.0+py3.11.5/lib/python3.11/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '../baselines/Qwen2.5-7B-Instruct'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "model_path = \"../../baselines/Qwen2.5-7B-Instruct\" # \"/gpfsdswork/dataset/HuggingFace_Models/meta-llama/Llama-2-7b-chat-hf\" #\"/lustre/fsn1/projects/rech/hjp/ulj12fo/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff7abc5-fc40-483c-b286-daecabc70a5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'NameError'>",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(\u001b[43mtokenizer\u001b[49m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m, skip_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce56a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, load_in_8bit=True, device_map=\"auto\",)  # torch_dtype=torch.float16, low_cpu_mem_usage=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e0770",
   "metadata": {},
   "source": [
    "## Format the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1349c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_text(premise, hypothesis):\n",
    "    # TODO the persona option + the wrapping of the pol + med \"patients with this medical profile...\"\n",
    "    options_prefix = \"Answer only with: \\n- \" #\"OPTIONS:\\n- \"\n",
    "    separator = \"\\n- \"\n",
    "    options_ = options_prefix + f\"{separator}\".join([\"Entailment\",\"Contradiction\"])  # , \"Neutral\"\n",
    "    return f\"{premise} \\n Question: Does the previous eligibility criteria imply that the following patient can participate to the trial?\\n {hypothesis}\\n {options_}\"\n",
    "    #return f\"{premise} \\n Question: Does this imply that {hypothesis}? {options_}\"\n",
    "    # Does the previous eligibility criteria imply that the following patient can participate to the trial?\n",
    "    # return f\"Classification: {premise} \\n Question: Does this imply that {hypothesis}? Entailment or Contradiction?Answer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c16fe5-f49f-4e7f-8973-9d5e01b55b8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307dd6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "# TODO adapt in function of the med or POL (l.5)\n",
    "for instance in subset:\n",
    "    premise = instance['premise']\n",
    "    sentence = f\"Eligibility criteria of the trial are:\\n {premise}\"\n",
    "    input_text = get_input_text(sentence, instance['statement_pol'])\n",
    "    # temp = {\"text\":input_text, \"label\":sample['label']}\n",
    "    temp = {\"text\":input_text, \"label\":0}\n",
    "    print(input_text)\n",
    "    samples.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400dd37",
   "metadata": {},
   "source": [
    "## Define the chat function\n",
    "\n",
    "TODO: adjust the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1a652-a0fe-484e-8271-4bdcdb359675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(\n",
    "    query: str,\n",
    "    history: Optional[List[Dict]] = None,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 1.0,\n",
    "    top_k: float = 0,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    max_new_tokens: int = 5, # 1024,\n",
    "    **kwargs,\n",
    "):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    history.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(history, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = input_ids.shape[1]\n",
    "\n",
    "    generated_outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            do_sample=temperature > 0.0,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            **kwargs,\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    generated_tokens = generated_outputs.sequences[0, input_length:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    history.append({\"role\": \"assistant\", \"content\": generated_text})\n",
    "\n",
    "    return generated_text, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00aa0f-26c9-488f-ad00-e3c984005e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = chat(samples[52]['text'], history=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972f607-bc87-4314-be7c-2394bdadbda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[52]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce8ee4",
   "metadata": {},
   "source": [
    "## Call the chat function on the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873f800-d556-4b6d-a453-37223706987b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "pred = []\n",
    "#with torch.inference_mode():\n",
    "for sample in tqdm.tqdm(samples):\n",
    "    labels.append(sample[\"label\"])\n",
    "    # input_ids = tokenizer(sample[\"text\"], return_tensors=\"pt\",).input_ids.to(\"cuda\")\n",
    "    # input_ids = tokenizer.apply_chat_template(sample[\"text\"], return_tensors=\"pt\",).to(\"cuda\")\n",
    "    # outputs = model.generate(input_ids, max_new_tokens=20)\n",
    "    response, history = chat(sample['text'], history=None)\n",
    "    pred.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708cd824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = [p[5:][:-4].strip() for p in pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c2bd7",
   "metadata": {},
   "source": [
    "## Parse model's output\n",
    "\n",
    "TODO: adapt the pattern in function of the model's output style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4a755-36f2-4e4b-8e1b-64226af0c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"entailment|contradiction|Entailment|Contradiction|entail\"  # |Neutral|neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15599cca-32ff-4583-8b26-ba4aff0a96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to match text with regular expression\n",
    "def match_text_with_regexp(text, pattern):\n",
    "    text = text.strip()\n",
    "    # print(text)\n",
    "    # Compile the regular expression pattern\n",
    "    regexp = re.compile(pattern)\n",
    "    \n",
    "    # Search for a match in the text\n",
    "    match = regexp.search(text)\n",
    "    # print(match)\n",
    "    \n",
    "    if match:\n",
    "        # If a match is found, return the matched text\n",
    "        return match.group()\n",
    "    else:\n",
    "        # If no match is found\n",
    "        return None\n",
    "\n",
    "parsed_preds = []\n",
    "\n",
    "for p in pred:\n",
    "    # text = pred[0]\n",
    "    pattern = \"entailment|contradiction|Entailment|Contradiction|entail\"  # |Neutral|neutral\n",
    "    \n",
    "    result = match_text_with_regexp(p, pattern)\n",
    "    \n",
    "    if result:\n",
    "        if result in ['entailment', 'entail', 'yes', 'Yes']:\n",
    "            result = 'Entailment'\n",
    "        elif result in ['contradiction', 'contradicts', 'no', 'No']:\n",
    "            result = 'Contradiction'\n",
    "        #elif result in ['neutral', 'Neutral']:\n",
    "            #result = 'Neutral'\n",
    "        # print(f\"Match found: {result}\")\n",
    "        parsed_preds.append(result) \n",
    "    else:\n",
    "        # print(\"No match found.\")\n",
    "        # if nothing exploitable predicted --> assert \"Neutral\"\n",
    "        parsed_preds.append(\"Contradiction\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5d3c3-c024-4d22-ba7a-a5ff85ff29ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d349880",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(parsed_preds)\n",
    "from collections import Counter\n",
    "Counter(parsed_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14285058",
   "metadata": {},
   "source": [
    "## Save the predictions in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict = {}\n",
    "for _id,pred_x in zip(data['train']['id'], parsed_preds):\n",
    "    prediction_dict[str(_id)] = {\"Prediction\":pred_x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfcff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(prediction_dict, open(\"results_qwen_2-5_7B_zs_pol.json\",'w'),indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.4.0_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.4.0_py3.11.5"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
